(dp1
S'output'
p2
S"<type 'exceptions.TypeError'> name must be an instance of basestring"
p3
sS'layer'
p4
S'D:\\Workspaces\\Tweet-Streaming-Data-Analysis-Service\\web2py\\applications\\twitter_analysis_app\\controllers/default.py'
p5
sS'code'
p6
S'# -*- coding: utf-8 -*-\n# this file is released under public domain and you can use without limitations\n\n# -------------------------------------------------------------------------\n# This is a sample controller\n# - index is the default action of any application\n# - user is required for authentication and authorization\n# - download is for downloading files uploaded in the db (does streaming)\n# -------------------------------------------------------------------------\nimport re\n\nimport pandas as pd\nimport tweepy\nfrom pymongo import MongoClient\n\nimport nltk\nfrom nltk.corpus import treebank\n\ncollection = MongoClient(\'localhost\', 27017)["tweets"]["StreamingDemo"]\n\n# collection = None\nconsumer_key = "8pCg0f2Ih81PSpbH5XQNptWPQ"\nconsumer_secret = "ghSdXGQKYABWdCn44TNizjvBt05l2mqeQtveBrBCkbVFGtB1iB"\n\naccess_token = "783900324704641024-fcqOlSez6zgnx3ArAq5jIDso736hT1V"\naccess_token_secret = "mXO8CSnhGP1jwBbBGop6zZTgmau4ywO7HtrF44MwBKAGZ"\n\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth)\nmyStream = None\n\n\ndef get_api():\n    consumer_key = "8pCg0f2Ih81PSpbH5XQNptWPQ"\n    consumer_secret = "ghSdXGQKYABWdCn44TNizjvBt05l2mqeQtveBrBCkbVFGtB1iB"\n\n    access_token = "783900324704641024-fcqOlSez6zgnx3ArAq5jIDso736hT1V"\n    access_token_secret = "mXO8CSnhGP1jwBbBGop6zZTgmau4ywO7HtrF44MwBKAGZ"\n\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_token, access_token_secret)\n\n    api = tweepy.API(auth)\n\n    return api\n\n\ndef get_collection():\n    collection = MongoClient(\'localhost\', 27017)["tweets"]["StreamingDemo"]\n    return collection\n\n\nclass MyStreamListener(tweepy.StreamListener):\n    global collection\n    global myStream\n    count = 1\n    goal = 1000 - collection.count()\n\n    def on_status(self, status):\n        self.count += 1\n        print self.count\n        collection.insert_one(status._json)\n        print collection\n        if self.count > 1000:\n            myStream.disconnect()\n            print "Twitter stream has been disconnected successfully."\n\nclass Keyword:\n    def __init__(self):\n        self.stopWordList = set()          # stop word list\n        f = open(\'StopWord.txt\', \'r\')\n        for line in f:\n            line = line[:-1]            # remove \'\\n\'\n            self.stopWordList.add(line)\n\n    def getKeyword(self, sentence):\n        words = []\n        tokens = nltk.word_tokenize(sentence)\n        tagged = nltk.pos_tag(tokens)\n\n        # extract noun\n        # for tag in tagged:\n        #     word = tag[0].lower()\n        #     if "NN" in tag[1] and word not in self.stopWordList:\n        #         words.append(word)\n\n        # extract noun and noun phrase\n        word = ""\n        index = 0\n        check = 0\n        while(index < len(tagged)):\n            if "NN" in tagged[index][1]:\n                word += tagged[index][0].lower()\n                check = index\n                while index+1 < len(tagged) and "NN" in tagged[index+1][1]:\n                    word += " " + tagged[index+1][0].lower()\n                    index += 1\n                words.append(word)\n            index += 1\n            word = ""\n\n        return words\n\ndef update_collection(search_word):\n    global myStream\n    print \'Start to update the collection :\'\n    myStream = tweepy.Stream(auth=api.auth, listener=MyStreamListener())\n    myStream.filter(track=[search_word])\n\n"""\ndef index():\n\n    example action using the internationalization operator T and flash\n    rendered by views/default/index.html or views/generic.html\n\n    if you need a simple wiki simply replace the two lines below with:\n    return auth.wiki()\n\n    collection = get_collection()\n    dataset = [{"created_at": item["created_at"],\n                "text": item["text"],\n                "user": "@%s" % item["user"]["screen_name"],\n                "source": item["source"],\n                "lang": item["lang"],\n                } for item in collection.find()]\n\n    dataset = pd.DataFrame(dataset)\n    dataset.source_name = dataset.source.apply(get_source_name)\n    source_counts = dataset.source_name.value_counts().sort_values()[-10:]\n    lang_counts = dataset.lang.value_counts().sort_values()\n    source_info = {}\n    for s in source_counts.iteritems():\n        source_info[s[0]] = s[1]\n    return dict(source_info=source_info)\n"""\n\ndef index():\n    return dict()\n\ndef get_source_name(x):\n    value = re.findall(pattern="<[^>]+>([^<]+)</a>", string=x)\n    if len(value) > 0:\n        return value[0]\n    else:\n        return ""\n\n\ndef get_chart_device():\n    global collection\n    # collection2 = MongoClient(\'localhost\', 27017)["tweets"]["Trump"]\n    # print \'The size of the collection is :\'\n    # print collection2.count()\n    search_word = request.vars.search_text\n    print search_word\n\n    collection= MongoClient(\'localhost\', 27017)["tweets"][search_word]\n    print collection.count()\n    if collection.count() == 0:\n        update_collection(search_word)\n        print \'End of updating collection :\'\n    print "get_chart_data"\n    # collection = get_collection()\n    dataset = [{"created_at": item["created_at"],\n                "text": item["text"],\n                "user": "@%s" % item["user"]["screen_name"],\n                "source": item["source"],\n                "lang": item["lang"],\n                "filter_level": item["filter_level"],\n                } for item in collection.find()]\n\n    dataset = pd.DataFrame(dataset)\n    dataset.source_name = dataset.source.apply(get_source_name)\n    source_counts = dataset.source_name.value_counts().sort_values()[-10:]\n    lang_counts = dataset.lang.value_counts().sort_values()[-5:]\n    filter_level_counts = dataset.filter_level.value_counts().sort_values()\n    print "------------ The language distribution is : ------------------"\n    print lang_counts\n    print "---------- End of the language distribution is : -------------"\n    print "------------ The filter_level distribution is : ------------------"\n    print filter_level_counts\n    print "---------- End of the filter_level distribution is : -------------"\n    data = []\n    for s in source_counts.iteritems():\n        data.insert(0, {\n            "Type": s[0],\n            "tweets": s[1].item()\n        })\n        print type(s[1].item())\n    print data\n    json_data = response.json(data)\n    print json_data\n    return response.json(data)\n    # json_data = json.dumps(data)\n    # print json_data\n    # return json.dumps(data)\n\n\ndef get_chart_lang():\n    print "get_chart_data"\n    collection = get_collection()\n    dataset = [{"created_at": item["created_at"],\n                "text": item["text"],\n                "user": "@%s" % item["user"]["screen_name"],\n                "source": item["source"],\n                "lang": item["lang"],\n                "filter_level": item["filter_level"],\n                } for item in collection.find()]\n\n    dataset = pd.DataFrame(dataset)\n    lang_counts = dataset.lang.value_counts().sort_values()[-5:]\n    filter_level_counts = dataset.filter_level.value_counts().sort_values()\n    print "------------ The language distribution is : ------------------"\n    print lang_counts\n    print "---------- End of the language distribution is : -------------"\n    print "------------ The filter_level distribution is : ------------------"\n    print filter_level_counts\n    print "---------- End of the filter_level distribution is : -------------"\n    data = []\n    for s in lang_counts.iteritems():\n        data.insert(0, {\n            "Type": s[0],\n            "tweets": s[1].item()\n        })\n        print type(s[1].item())\n    print data\n    json_data = response.json(data)\n    print json_data\n    return response.json(data)\n    # json_data = json.dumps(data)\n    # print json_data\n    # return json.dumps(data)\n\n\ndef get_chart_hashtag():\n    top_k_hashtag = {}\n    for item in collection.find():\n        try:\n            hashtags = item[\'entities\'][\'hashtags\']\n            print hashtags\n            for hashtag in hashtags:\n                text = hashtag[\'text\'].lower()\n                print text\n                if text not in top_k_hashtag:\n                    top_k_hashtag[text] = 1\n                    print \'---1---\'\n                else:\n                    print \'---+---\'\n                    temp = top_k_hashtag.get(text)\n                    temp += 1\n                    top_k_hashtag[text] = temp\n        except:\n            print \'error happens\'\n            pass\n    top_tree = sorted(top_k_hashtag.iteritems(), key=lambda x: x[1], reverse=True)\n    data = []\n    for item in top_tree[:10]:\n        data.insert(0, {\n            "Type": item[1],\n            "tweets": str(item[0])\n        })\n    print data\n    return \'ok\'\n\n\ndef process_location_analysis():\n    global collection\n\n    print \'Start location analysis : \'\n    print collection.count()\n\n    dataset = [{"source": item["source"]\n                } for item in collection.find()]\n\n    dataset = pd.DataFrame(dataset)\n    dataset.source_name = dataset.source.apply(get_source_name)\n    result = dataset.source_name.value_counts().sort_values()[-10:]\n\n    data = []\n    for s in result.iteritems():\n        data.insert(0, {\n            "Type": s[0],\n            "tweets": s[1].item()\n        })\n\n    return data\n\n\ndef process_language_analysis():\n    global collection\n\n    dataset = [{"lang": item["lang"]\n                } for item in collection.find()]\n\n    dataset = pd.DataFrame(dataset)\n    result = dataset.lang.value_counts().sort_values()[-5:]\n\n    data = []\n    for s in result.iteritems():\n        data.insert(0, {\n            "Type": s[0],\n            "tweets": s[1].item()\n        })\n\n    return data\n\n\n"""\ndef process_hashtag_analysis():\n    global collection\n\n    top_k_hashtag = {}\n    for item in collection.find():\n        try:\n            hashtags = item[\'entities\'][\'hashtags\']\n            print \'Found the hashtag.\'\n            print hashtags\n            for hashtag in hashtags:\n                text = hashtag[\'text\'].lower()\n                print text\n                if text not in top_k_hashtag:\n                    top_k_hashtag[text] = 1\n                    print \'111111111\'\n                else:\n                    print \'2222222222\'\n                    temp = top_k_hashtag[text]\n                    print \'The current is : \' + temp\n                    temp = temp + 1\n                    top_k_hashtag[text] = temp\n        except:\n            print \'error happens\'\n            pass\n\n    data = []\n    top_sorted = sorted(top_k_hashtag.iteritems(), key=lambda x: x[1], reverse=True)\n    for item in top_sorted[:10]:\n        data.insert(0, {\n            "Type": str(item[0]),\n            "tweets": item[1]\n        })\n\n    return data\n"""\n\n\ndef process_hashtag_analysis():\n    global collection\n\n    top_k_hashtag = {}\n    for item in collection.find():\n        try:\n            hashtags = item[\'entities\'][\'hashtags\']\n            print \'Found the hashtag.\'\n            print hashtags\n            for hashtag in hashtags:\n                text = hashtag[\'text\'].lower()\n                print text\n                if text not in top_k_hashtag:\n                    top_k_hashtag[text] = 1\n                    print \'111111111\'\n                else:\n                    print \'2222222222\'\n                    temp = top_k_hashtag[text]\n                    print \'The current is : \' + str(temp)\n                    temp = temp + 1\n                    top_k_hashtag[text] = temp\n        except Exception as e:\n            print e.__doc__\n            print e.message\n            pass\n\n    data = []\n    top_sorted = sorted(top_k_hashtag.iteritems(), key=lambda x: x[1], reverse=False)\n    for item in top_sorted[-10:]:\n        data.insert(0, {\n            "Type": str(item[0]),\n            "tweets": item[1]\n        })\n\n    return data\n\n\ndef process_keyword_analysis():\n    global collection\n    print collection.count()\n    print collection\n    keyword = Keyword()\n    data = []\n\n    return data\n\n\n"""\ndef process_keyword_analysis():\n    global collection\n\n    print \'0\'\n    key_word = Keyword()\n    print \'--\'\n    word_frequency = {}\n    print \'1\'\n    for item in collection.find():\n        parsed_word = Keyword.getKeyword(key_word, item[\'text\'])\n        print \'2\'\n        for unicode_word in parsed_word:\n            try:\n                word = str(unicode_word)\n            except (UnicodeEncodeError):\n                pass\n            if word_frequency.has_key(word):\n                word_frequency[word] += 1\n            else:\n                word_frequency[word] = 1\n\n    data = []\n    top_sorted = sorted(word_frequency.iteritems(), key=lambda x: x[1], reverse=False)\n    for item in top_sorted[-10:]:\n        data.insert(0, {\n            "Type": str(item[0]),\n            "tweets": item[1]\n        })\n\n    return data\n"""\n\ndef get_chart_data():\n    global collection\n\n    search_word = request.vars.search_text\n    analysis_type = request.vars.type_of_analysis\n    print search_word\n    print analysis_type\n\n    collection = MongoClient(\'localhost\', 27017)["tweets"][search_word]\n    print collection.count()\n    if collection.count() < 1000:\n        update_collection(search_word)\n        print \'End of updating collection :\'\n    print "get_chart_data start : "\n\n    data = []\n    if analysis_type == \'Location\':\n        data = process_location_analysis()\n    elif analysis_type == \'Language\':\n        data = process_language_analysis()\n    elif analysis_type == \'Hashtag\':\n        data = process_hashtag_analysis()\n    elif analysis_type == \'Keyword\':\n        print \'start to analysis keyword.\'\n        data = process_keyword_analysis()\n\n    json_data = response.json(data)\n\n    print json_data\n    return response.json(data)\n    # json_data = json.dumps(data)\n    # print json_data\n    # return json.dumps(data)\n\ndef user():\n    """\n    exposes:\n    http://..../[app]/default/user/login\n    http://..../[app]/default/user/logout\n    http://..../[app]/default/user/register\n    http://..../[app]/default/user/profile\n    http://..../[app]/default/user/retrieve_password\n    http://..../[app]/default/user/change_password\n    http://..../[app]/default/user/bulk_register\n    use @auth.requires_login()\n        @auth.requires_membership(\'group name\')\n        @auth.requires_permission(\'read\',\'table name\',record_id)\n    to decorate functions that need access control\n    also notice there is http://..../[app]/appadmin/manage/auth to allow administrator to manage users\n    """\n    return dict(form=auth())\n\n\n@cache.action()\ndef download():\n    """\n    allows downloading of uploaded files\n    http://..../[app]/default/download/[filename]\n    """\n    return response.download(request, db)\n\n\ndef call():\n    """\n    exposes services. for example:\n    http://..../[app]/default/call/jsonrpc\n    decorate with @services.jsonrpc the functions to expose\n    supports xml, json, xmlrpc, jsonrpc, amfrpc, rss, csv\n    """\n    return service()\n\n\n\nresponse._vars=response._caller(get_chart_data)\n'
p7
sS'snapshot'
p8
(dp9
sS'traceback'
p10
S'Traceback (most recent call last):\n  File "D:\\Workspaces\\Tweet-Streaming-Data-Analysis-Service\\web2py\\gluon\\restricted.py", line 227, in restricted\n    exec ccode in environment\n  File "D:\\Workspaces\\Tweet-Streaming-Data-Analysis-Service\\web2py\\applications\\twitter_analysis_app\\controllers/default.py", line 501, in <module>\n  File "D:\\Workspaces\\Tweet-Streaming-Data-Analysis-Service\\web2py\\gluon\\globals.py", line 417, in <lambda>\n    self._caller = lambda f: f()\n  File "D:\\Workspaces\\Tweet-Streaming-Data-Analysis-Service\\web2py\\applications\\twitter_analysis_app\\controllers/default.py", line 436, in get_chart_data\n    collection = MongoClient(\'localhost\', 27017)["tweets"][search_word]\n  File "D:\\DevTools\\Anaconda2\\lib\\site-packages\\pymongo\\database.py", line 233, in __getitem__\n    return Collection(self, name)\n  File "D:\\DevTools\\Anaconda2\\lib\\site-packages\\pymongo\\collection.py", line 144, in __init__\n    "of %s" % (string_type.__name__,))\nTypeError: name must be an instance of basestring\n'
p11
s.